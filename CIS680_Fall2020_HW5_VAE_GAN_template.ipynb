{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr1jhNq8Oiab"
      },
      "source": [
        "Download the support file for IS computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFTAlD02OAj5",
        "outputId": "e54afaca-47b1-4e75-eaa4-c5e17dcfa6d5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from googleapiclient.discovery import build\n",
        "# drive_service = build('drive', 'v3')\n",
        "# import io\n",
        "# import os\n",
        "# from googleapiclient.http import MediaIoBaseDownload\n",
        "# def download_file(fn, file_id):\n",
        "#     request = drive_service.files().get_media(fileId=file_id)\n",
        "#     downloaded = io.BytesIO()\n",
        "#     downloader = MediaIoBaseDownload(downloaded, request)\n",
        "#     done = False\n",
        "#     while done is False:\n",
        "#         # _ is a placeholder for a progress object that we ignore.\n",
        "#         # (Our file is small, so we skip reporting progress.)\n",
        "#         _, done = downloader.next_chunk()\n",
        "#     downloaded.seek(0)\n",
        "#     folder = fn.split('/')\n",
        "#     if len(folder) > 1:\n",
        "#         os.makedirs(folder[0], exist_ok=True)\n",
        "#     with open(fn, 'wb') as f:\n",
        "#         f.write(downloaded.read())\n",
        "# id_to_fn = {\n",
        "# '1z-3wIa4MzQp2fHHn2_1v1ZuI8sYKXrTw': 'inception_score.py',\n",
        "# '1ijo3v_Y-XrV8ytpWRey9PvHy_NV9cTBE': 'test_case_VAE.zip',\n",
        "# '1FZyHINMFqELoTjWUHSwM3krMF1bH3ROV': 'test_case_GAN.zip'\n",
        "# }\n",
        "# # download all files into the vm\n",
        "# for fid, fn in id_to_fn.items():\n",
        "#     print(\"Downloading %s from %s\" % (fn, fid))\n",
        "#     download_file(fn, fid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGskxwSyNHM7"
      },
      "outputs": [],
      "source": [
        "# ! unzip -q test_case_VAE.zip\n",
        "# ! unzip -q test_case_GAN.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnqQeI6zqpPE"
      },
      "source": [
        "# VAE\n",
        "### Load data for part 1.1 VAE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-j7RwOOq4UH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from scipy import linalg\n",
        "from scipy.stats import entropy\n",
        "import tqdm\n",
        "import cv2\n",
        "\n",
        "# FashionMNIST Dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_batch_size = 100\n",
        "test_batch_size = 100\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Viewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a batch of images\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Create a grid of images\n",
        "grid_image = vutils.make_grid(images, nrow=8, padding=2, normalize=True)\n",
        "\n",
        "# Convert and Display the grid image\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(grid_image.permute(1, 2, 0).numpy())\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Grid of MNIST Images\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj4AdywfsuoI"
      },
      "source": [
        "## Model Definition (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQdT4OYqsjlJ",
        "outputId": "d862210a-9eae-40ae-9e09-1672d536bf52"
      },
      "outputs": [],
      "source": [
        "class VAE(torch.nn.Module):\n",
        "  def __init__(self, zdim):\n",
        "    super(VAE,self).__init__()\n",
        "    ################################\n",
        "    # Please fill in your code here:\n",
        "\n",
        "    ################################\n",
        "    \n",
        "    \n",
        "  def encode(self, X):\n",
        "    ################################\n",
        "    # Please fill in your code here:\n",
        "\n",
        "    ################################\n",
        "    return mean, log_var\n",
        "\n",
        "  def decode(self, X):\n",
        "    ################################\n",
        "    # Please fill in your code here:\n",
        "\n",
        "    ################################\n",
        "    return X\n",
        "\n",
        "  def reparameterization(self, mean, log_var):\n",
        "    ################################\n",
        "    # Please fill in std, eps and z:\n",
        "    # std = \n",
        "    # eps = \n",
        "    # z = \n",
        "    ################################\n",
        "\n",
        "    return z\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = X.view(-1,784)\n",
        "    mean, log_var = self.encode(X)\n",
        "    z = self.reparameterization(mean, log_var)\n",
        "    return self.decode(z), mean, log_var\n",
        "\n",
        "\n",
        "\n",
        "# reparameterization's output is dynamic, for the test case, we use a fixed eps \n",
        "# and all the intermediate result is provided. \n",
        "# You could use these values to check if you get the final output z correct.\n",
        "# Or you could add eps to the input when testing the reparameterization module. \n",
        "# (warm reminder: don't forget to change back, cause the dynamic reparameterization is the key to VAE) \n",
        "\n",
        "# TEST YOUR REPARAMETRIZATION FUNCTION with the values below\n",
        "testcase_mean = torch.load('test_case_VAE/mean.pt')\n",
        "testcase_log_var = torch.load('test_case_VAE/log_var.pt')\n",
        "# check std\n",
        "testcase_std = torch.load('test_case_VAE/std.pt')\n",
        "# Since epsilon is random, use the deterministic value of epsilon provided below\n",
        "testcase_eps = torch.load('test_case_VAE/eps.pt')\n",
        "testcase_z = torch.load('test_case_VAE/z.pt' )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3sf0UMAthIt"
      },
      "source": [
        "## VAE Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFdwstCq3ge"
      },
      "source": [
        "### VAE Reconstruction Evaluation (TODO)\n",
        "For this reconstruction evaluation module, you need to integrate into training procesure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8yGCaiAJock"
      },
      "outputs": [],
      "source": [
        "# Reconstruction error module\n",
        "def reconstruction_error(model, test_loader):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        model: VAE model\n",
        "        test_loader: Fashion-MNIST test_loader\n",
        "    Output:\n",
        "        avg_err: MSE \n",
        "    '''\n",
        "    # set model to eval\n",
        "    ##################\n",
        "    # TODO:\n",
        "    ##################\n",
        "    # Initialize MSE Loss(use reduction='sum')\n",
        "    ##################\n",
        "    # TODO:\n",
        "    ##################\n",
        "    recon_err = 0\n",
        "    idx_counter = 0\n",
        "    for i, (data,_) in enumerate(test_loader):\n",
        "        data = data.to(device)\n",
        "        # feed forward data to VAE\n",
        "        ##################\n",
        "        # TODO:\n",
        "        ##################\n",
        "        \n",
        "        idx_counter+=data.shape[0] # sum up the number of images in test_loader\n",
        "\n",
        "        # flatten the reconstruction output\n",
        "        ##################\n",
        "        # TODO:\n",
        "        ##################\n",
        "        # accumulate the MSELoss acrossing the whole test set\n",
        "        ##################\n",
        "        # TODO:\n",
        "        ##################\n",
        "        \n",
        "    avg_err = recon_err/idx_counter\n",
        "    return avg_err\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SZQgumEpGCo"
      },
      "source": [
        "### Reconstruction error + KL divergence losses in VAE (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSeUxvFjpCTE"
      },
      "outputs": [],
      "source": [
        "# Return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    '''\n",
        "    Compute reconstruction loss and KL divergence loss mentioned in pdf handout\n",
        "    '''\n",
        "    ################################\n",
        "    # Please compute BCE and KLD:\n",
        "    \n",
        "    ################################\n",
        "    totalloss = BCE + KLD\n",
        "\n",
        "    return totalloss\n",
        "\n",
        "\n",
        "#####################################################\n",
        "# TEST CASE FOR VAE LOSS\n",
        "#####################################################\n",
        "testcase_loss_recon_x = torch.load('test_case_VAE/loss_recon_x.pt')\n",
        "testcase_loss_x = torch.load('test_case_VAE/loss_x.pt')\n",
        "testcase_loss_mu = torch.load('test_case_VAE/loss_mu.pt')\n",
        "testcase_loss_log_var = torch.load('test_case_VAE/loss_log_var.pt')\n",
        "testcase_loss_totalloss = torch.load('test_case_VAE/loss_totalloss.pt')\n",
        "\n",
        "loss = loss_function(testcase_loss_recon_x, testcase_loss_x, testcase_loss_mu, testcase_loss_log_var)\n",
        "print(\"test case loss value:\", testcase_loss_totalloss.item())\n",
        "print(\"computed loss value:\", loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptpewb72pLXD"
      },
      "source": [
        "### VAE Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0mwNeKftmn2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Z dimension\n",
        "ZDIM = 5\n",
        "\n",
        "#Initialize VAE\n",
        "vae = VAE(ZDIM).to(device)\n",
        "#Initialize optimizer\n",
        "optimizer = optim.Adam(vae.parameters(), lr = 1e-3)\n",
        "#Initialize scheduler(optional)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "#num of epochs \n",
        "num_epochs = 10\n",
        "import pdb\n",
        "train_loss_list = []\n",
        "orig_image_list = []\n",
        "recon_image_list = []\n",
        "reconst_error = []\n",
        "\n",
        "\n",
        "# Define Train loop \n",
        "def train(epochs, train_loader, test_loader):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      vae.train()\n",
        "      train_loss = 0\n",
        "      print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
        "      for batch_idx, (data, _) in enumerate(train_loader):\n",
        "          data = data.cuda()\n",
        "          optimizer.zero_grad()\n",
        "          recon_batch, mean, log_var = vae(data)\n",
        "\n",
        "          loss = loss_function(recon_batch, data, mean, log_var)\n",
        "          \n",
        "          loss.backward()\n",
        "          train_loss += loss.item()\n",
        "          optimizer.step()\n",
        "\n",
        "          if batch_idx % 100 == 0:\n",
        "              recon_err = reconstruction_error(vae, test_loader)\n",
        "              reconst_error.append(recon_err)\n",
        "              print('Train Epoch: {} {:.0f}% \\tLoss: {:.6f} \\tRecon_err: {}'.format(epoch+1, 100. * batch_idx / len(train_loader), loss.item() / len(data), recon_err))\n",
        "          del data; del recon_batch; del mean; del log_var    \n",
        "          \n",
        "      train_loss_list.append(train_loss / len(train_loader.dataset))\n",
        "      print('Epoch: {} Train loss: {:.4f}'.format(epoch+1, train_loss / len(train_loader.dataset)))\n",
        "      scheduler.step() \n",
        "\n",
        "    #   recon_err = reconstruction_error(vae, test_loader)\n",
        "    #   print('Epoch: {} Reconstruction Error: {:.4f}'.format(epoch+1, recon_err))\n",
        "      if epoch%5==0:\n",
        "        with torch.no_grad():\n",
        "          \n",
        "            x_batch =torch.randn(10*10, ZDIM)\n",
        "            recon_batch = vae.decode(x_batch.to(device))\n",
        "\n",
        "        orig_image_list.append(vutils.make_grid(x_batch, nrow=10 ,padding=2, normalize=True))\n",
        "        recon_image_list.append(vutils.make_grid(recon_batch.view(recon_batch.shape[0], 1 , 28, 28).detach().cpu(),nrow=10 , padding=2, normalize=True))\n",
        "\n",
        "  # save the training checkpoint\n",
        "  checkpoint = {'vae': vae.state_dict()}\n",
        "  torch.save(checkpoint, '/content/vae_{}.pt'.format(epochs))\n",
        "# Run Train loop\n",
        "train(num_epochs, train_loader, test_loader)\n",
        "\n",
        "# Plot Train loss\n",
        "plt.title(\"VAE Train Loss\")\n",
        "plt.plot(train_loss_list,label=\"train loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxC5skbrobbu"
      },
      "source": [
        "## Qualitative Visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u40nhedEzdI1"
      },
      "source": [
        "### VAE Testing (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6KJ-Gh5Lzbyi",
        "outputId": "6a2dc405-2241-4374-cb13-dd15059d3831"
      },
      "outputs": [],
      "source": [
        "# Test VAE on a random sample and display on 6X6 grid\n",
        "grid_size = 6 \n",
        "\n",
        "################################\n",
        "# Please fill in your code here:\n",
        "\n",
        "################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cux4fknS4kJT"
      },
      "source": [
        "### Visualize the original vs reconstructed images (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFQCfG1am7Tz"
      },
      "outputs": [],
      "source": [
        "# Visualise the original vs reconstructed images\n",
        "# Input the 36 images in Fashion-MNIST to VAE network \n",
        "# and display reconstructed output on 6X6 grid\n",
        "\n",
        "################################\n",
        "# Please fill in your code here:\n",
        "\n",
        "################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMLfSt-V4rlC"
      },
      "source": [
        "## Quantitative Evaluation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp4FZlAHpZf3"
      },
      "outputs": [],
      "source": [
        "# Please plot out the reconstruction error curve you saved \n",
        "# in 'reconstruction_error' list in training.\n",
        "# And report the final reconstruction error value\n",
        "################################\n",
        "# Please fill in your code here:\n",
        "\n",
        "################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvlASdoVyKF2"
      },
      "source": [
        "### VAE IS score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04qWlBhUN3n1"
      },
      "source": [
        "The following block create 2 torch dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpmE6zrjyJa6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_saved = torch.load('/content/vae_{}.pt'.format(num_epochs))\n",
        "model_vae = VAE(ZDIM)\n",
        "model_vae.load_state_dict(model_saved['vae'])\n",
        "\n",
        "# Prepare the torch dataset\n",
        "evaluate_num = 1000\n",
        "real_set = []\n",
        "gen_set = []\n",
        "test_batch_size = 1\n",
        "test_dataset = datasets.FashionMNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "for idx, data in enumerate(test_loader, 0):\n",
        "    model_vae.eval()\n",
        "    image = data[0]\n",
        "    # print(image.shape)\n",
        "    gen_image, _, _ = model_vae(image)\n",
        "\n",
        "    # print(gen_image.shape)\n",
        "    gen_image = gen_image.reshape(-1,28,28).unsqueeze(0)\n",
        "    # print(gen_image.shape)\n",
        "    \n",
        "    image = image.repeat(1,3,1,1)\n",
        "    gen_image = gen_image.repeat(1,3,1,1)\n",
        "    real_set.append(image)\n",
        "    gen_set.append(gen_image)\n",
        "\n",
        "        \n",
        "    if idx == evaluate_num-1:\n",
        "        break\n",
        "\n",
        "# make the pytorch dataset\n",
        "real_set = TensorDataset(torch.cat(real_set))\n",
        "gen_set = TensorDataset(torch.cat(gen_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ijLcZfzQdIB"
      },
      "source": [
        "#### Self-implement IS score (optional TODO)\n",
        "This optional part is for implementing IS score computation following pdf instructions. You could also use the the next section \"IS API\" to compute the IS score.\n",
        "For implementation, you would be asked to implement the most important part \"compute_IS\" function as an understanding of IS score computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9oiBmFfoVd7"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.inception import inception_v3\n",
        "\n",
        "def build_feature_table(dataset, model, batch_size, dim, device, upsample):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        model: Inception network v3\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        upsample: Inception network v3 only support input with resolution 299x299\n",
        "    Output:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    '''\n",
        "    # model enter eval mode\n",
        "    model.eval()\n",
        "    # initalize the dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "    n = len(dataset)\n",
        "    idx_counter = 0\n",
        "    # feature table\n",
        "    feature_table = np.zeros((n, dim))\n",
        "\n",
        "    for i, data in tqdm.tqdm(enumerate(dataloader, 0)):\n",
        "        image = data[0].to(device)\n",
        "        up = nn.Upsample(size=(299, 299), mode='bilinear')\n",
        "        with torch.no_grad():\n",
        "            if upsample == True:\n",
        "                # upsample to 299x299 resolution\n",
        "                pred = up(image)\n",
        "            else:\n",
        "                pred = image\n",
        "            pred = model(pred)\n",
        "            # print(pred.shape)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "            \n",
        "            feature_table[idx_counter:idx_counter+pred.shape[0]] = pred\n",
        "            idx_counter += len(pred)\n",
        "        del image\n",
        "    assert feature_table.shape == (n, dim)\n",
        "    return feature_table\n",
        "\n",
        "def compute_IS(feature_table):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    Output:\n",
        "        IS_score: float\n",
        "    '''\n",
        "    ################################\n",
        "    # Please fill in your code here:\n",
        "\n",
        "    ################################\n",
        "    return IS_score\n",
        "\n",
        "def IS(dataset, device, upsample=True, batch_size=64, dim=1000):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        upsample: Inception network v3 only support input with resolution 299x299\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "    Output:\n",
        "        IS_score: float\n",
        "    '''\n",
        "    \n",
        "    # load InveptionV3 model\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "\n",
        "    ## build up the feature table \n",
        "    feature_table = build_feature_table(dataset, model, batch_size, dim, device, upsample)\n",
        "\n",
        "    ## IS score computation\n",
        "    IS_score = compute_IS(feature_table)\n",
        "    \n",
        "    \n",
        "    return IS_score\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJDgXLWxOGF2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# compute IS score\n",
        "real_set_IS_score = IS(real_set, device, upsample = True)\n",
        "gen_set_IS_score = IS(gen_set, device, upsample = True)\n",
        "print('real images IS_score:', real_set_IS_score)\n",
        "print('generated images IS_score:', gen_set_IS_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKGddQbzNyMt"
      },
      "source": [
        "#### IS API\n",
        "Here you could apply the existed API to compute IS score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtGboKu3Nw8Z"
      },
      "outputs": [],
      "source": [
        "from inception_score import inception_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "print('real images IS_score:')\n",
        "print(inception_score(real_set, cuda=True, batch_size=64, resize=True, splits=1))\n",
        "\n",
        "print('generated images IS_score:')\n",
        "print(inception_score(gen_set, cuda=True, batch_size=64, resize=True, splits=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ThKTFe5qfq"
      },
      "source": [
        "# GAN (please restart you colab session to refresh the GPU memory usage)\n",
        "### Load data for part 2.1 GAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDu-cCsd5yVn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from scipy import linalg\n",
        "from scipy.stats import entropy\n",
        "import tqdm\n",
        "import cv2\n",
        "# Resize image to this size\n",
        "image_size=64\n",
        "\n",
        "# Setting up transforms to resize and normalize \n",
        "transform=transforms.Compose([ transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# batchsize of dataset\n",
        "batch_size = 128\n",
        "\n",
        "# Load STL-10 Dataset\n",
        "gan_train_dataset = datasets.STL10(root='./stl10_data/', split='train', transform=transform, download=True)\n",
        "gan_train_loader = torch.utils.data.DataLoader(dataset=gan_train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SEiJ7gT-ceC"
      },
      "source": [
        "## Model Definition (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzTNviQm-dB_"
      },
      "outputs": [],
      "source": [
        "# class DCGAN_Generator(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(DCGAN_Generator,self).__init__()\n",
        "#         ################################\n",
        "#         # Please fill in your code here:\n",
        "#         # self.layer =  nn.Sequential()\n",
        "#         ################################\n",
        "#         self.tConv1 = nn.ConvTranspose2d(100, 1024, kernel_size=(4,4), stride=1, padding = 0)\n",
        "#         self.bn1 = nn.BatchNorm2d(1024)\n",
        "\n",
        "#         self.tConv2 = nn.ConvTranspose2d(1024, 512, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.bn2 = nn.BatchNorm2d(512)\n",
        "\n",
        "#         self.tConv3 = nn.ConvTranspose2d(512, 256, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.bn3 = nn.BatchNorm2d(256)\n",
        "        \n",
        "#         self.tConv4 = nn.ConvTranspose2d(256, 128, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.bn4 = nn.BatchNorm2d(128)        \n",
        "\n",
        "#         self.tConv5 = nn.ConvTranspose2d(128, 3, kernel_size=(4,4), stride=2, padding = 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.tConv1(x)\n",
        "#         x = F.relu(self.bn1(x))\n",
        "\n",
        "#         x = self.tConv2(x)\n",
        "#         x = F.relu(self.bn2(x))\n",
        "\n",
        "#         x = self.tConv3(x)\n",
        "#         x = F.relu(self.bn3(x))\n",
        "\n",
        "#         x = self.tConv4(x)\n",
        "#         x = F.relu(self.bn4(x))\n",
        "\n",
        "#         x = self.tConv5(x)\n",
        "#         return F.tanh(x)\n",
        "\n",
        "# class DCGAN_Discriminator(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(DCGAN_Discriminator, self).__init__()\n",
        "#         ################################\n",
        "#         # Please fill in your code here:\n",
        "#         # self.layer =  nn.Sequential()\n",
        "#         ################################\n",
        "#         self.cn1 = nn.Conv2d(3, 128, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.cn2 = nn.Conv2d(128, 256, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.bn2 = nn.BatchNorm2d(256)\n",
        "#         self.cn3 = nn.Conv2d(256, 512, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.bn3 = nn.BatchNorm2d(512)\n",
        "#         self.cn4 = nn.Conv2d(512, 1024, kernel_size=(4,4), stride=2, padding = 1)\n",
        "#         self.bn3 = nn.BatchNorm2d(1024)\n",
        "#         self.cn5 = nn.Conv2d(1024, 1, kernel_size=(4,4), stride=1, padding = 0)\n",
        "\n",
        "#         self.lR = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        \n",
        "    \n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.cn1(x)\n",
        "#         x = self.lR(x)\n",
        "\n",
        "#         x = self.cn2(x)\n",
        "#         x = self.lR(self.bn2(x))\n",
        "\n",
        "#         x = self.cn3(x)\n",
        "#         x = self.lR(self.bn3(x))\n",
        "\n",
        "#         x = self.cn4(x)\n",
        "#         x = self.lR(self.bn4(x))\n",
        "\n",
        "#         x = self.cn5(x)\n",
        "\n",
        "#         return F.sigmoid(x)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DCGAN_Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DCGAN_Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            # tConv-BN-ReLU Block 1\n",
        "            nn.ConvTranspose2d(100, 1024, kernel_size=(4,4), stride=1, padding = 0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # tConv-BN-ReLU Block 2\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=(4,4), stride=2, padding = 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # tConv-BN-ReLU Block 3\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=(4,4), stride=2, padding = 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # tConv-BN-ReLU Block 4\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=(4,4), stride=2, padding = 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # tConv-tanh Block 5\n",
        "            nn.ConvTranspose2d(128, 3, kernel_size=(4,4), stride=2, padding = 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class DCGAN_Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DCGAN_Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            # Conv-LeakyReLU Block 1\n",
        "            nn.Conv2d(3, 128, kernel_size=(4,4), stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            # Conv-BN-LeakyReLU Block 2\n",
        "            nn.Conv2d(128, 256, kernel_size=(4,4), stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Conv-BN-LeakyReLU Block 3\n",
        "            nn.Conv2d(256, 512, kernel_size=(4,4), stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            # Conv-BN-LeakyReLU Block 4\n",
        "            nn.Conv2d(512, 1024, kernel_size=(4,4), stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Conv-Sigmoid Block 5\n",
        "            nn.Conv2d(1024, 1, kernel_size=(4,4), stride=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib3K2Vbt-krm"
      },
      "source": [
        "## GAN Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOKSYlB9N2Kc"
      },
      "source": [
        "### GAN loss (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdgHtRsZN0LL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "fake = torch.load('test_case_GAN/fake.pt')\n",
        "netD = torch.load('test_case_GAN/netD.pt')\n",
        "real = torch.load('test_case_GAN/real.pt')\n",
        "netG = torch.load('test_case_GAN/netG.pt')\n",
        "noise = torch.load('test_case_GAN/noise.pt')\n",
        "Valid_label = torch.load('test_case_GAN/Valid_label.pt')\n",
        "Fake_label = torch.load('test_case_GAN/Fake_label.pt')\n",
        "criterion = torch.load('test_case_GAN/criterion.pt')\n",
        "\n",
        "def loss_discriminator(D, real, G, noise, Valid_label, Fake_label, criterion):\n",
        "    '''\n",
        "    Chutyo input kya diya wo to bol do\n",
        "    D -> Discriminator Model \n",
        "    real -> Real images (3,64,64)\n",
        "    G -> Generator Model\n",
        "    noise -> I believe an input? (100,1,1)\n",
        "    Valid_label -> 128\n",
        "    Fake_label -> 128\n",
        "    criterion -> BCELoss function\n",
        "\n",
        "    ##\n",
        "    1. Forward real images into the discriminator\n",
        "    2. Compute loss between Valid_label and dicriminator output on real images\n",
        "    3. Forward noise into the generator to get fake images\n",
        "    4. Forward fake images to the discriminator\n",
        "    5. Compute loss between Fake_label and discriminator output on fake images\n",
        "    6. sum real loss and fake loss as the loss_D\n",
        "    7. we also need to output fake images generate by G(noise) for loss_generator computation\n",
        "    '''\n",
        "    # Fwd real images into discriminator\n",
        "    dLab = D(real).squeeze()\n",
        "    # Compute loss between Valid label and Discriminator\n",
        "    loss_d = criterion(dLab, Valid_label)\n",
        "    # Forward noise into generator and then discriminator \n",
        "    gOut = G(noise)\n",
        "    gLab = D(gOut).squeeze()        ## I wonder how will I handle this in a better way\n",
        "    # Compute loss between Fake and Discriminator\n",
        "    loss_g = criterion(gLab, Fake_label)\n",
        "\n",
        "    #Sum real loss and fake loss\n",
        "    loss_D = loss_g + loss_d    \n",
        "    # return total loss_D and fake images\n",
        "\n",
        "    return loss_D, gOut\n",
        "\n",
        "def loss_generator(netD, fake, Valid_label, criterion):\n",
        "    '''\n",
        "    1. Forward fake images to the discriminator\n",
        "    2. Compute loss between valid labels and discriminator output on fake images\n",
        "    '''\n",
        "    # Fwd Fake images to D\n",
        "    fake_D = netD(fake).squeeze()\n",
        "\n",
        "    # Loss between Valid Labels and D\n",
        "    loss_G = criterion(fake_D, Valid_label)\n",
        "    #return loss_G\n",
        "    return loss_G\n",
        "\n",
        "\n",
        "loss_D, fake_G = loss_discriminator(netD, real, netG, noise, Valid_label, Fake_label, criterion)\n",
        "torch.save(loss_D, 'test_case_GAN/loss_D.pt')\n",
        "loss_G = loss_generator(netD, fake, Valid_label, criterion)\n",
        "torch.save(loss_G, 'test_case_GAN/loss_G.pt')\n",
        "\n",
        "\n",
        "\n",
        "test_loss_D = torch.load('test_case_GAN/loss_D.pt')\n",
        "test_loss_G = torch.load('test_case_GAN/loss_G.pt')\n",
        "\n",
        "print('test case loss_D:', test_loss_D.item())\n",
        "print('computed loss_D:', loss_D.item())\n",
        "\n",
        "print('test case loss_G:', test_loss_G.item())\n",
        "print('computed loss_G:', loss_G.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iBE_CdbOQ4j"
      },
      "source": [
        "### Training block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nElhea1z-pMe"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pdb\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Number of channels\n",
        "nc = 3\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "# Size of feature maps in generator\n",
        "ngf = 128\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 128\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Create the generator and discriminator\n",
        "netG = DCGAN_Generator().to(device)\n",
        "netD = DCGAN_Discriminator().to(device)\n",
        "\n",
        "# Apply weight initialization\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create latent vector to test the generator performance\n",
        "fixed_noise = torch.randn(36, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "learning_rate = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "\n",
        "\n",
        "# Learning rate schedulers for both G and D\n",
        "schedulerD = StepLR(optimizerD, step_size=50, gamma=0.5)  \n",
        "schedulerG = StepLR(optimizerG, step_size=50, gamma=0.5)\n",
        "\n",
        "\n",
        "img_list = []\n",
        "real_img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "num_epochs = 120\n",
        "\n",
        "\n",
        "# GAN Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(gan_train_loader, 0):\n",
        "        real = data[0].to(device)\n",
        "        b_size = real.size(0)\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "\n",
        "        Valid_label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        Fake_label = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        netD.zero_grad()\n",
        "        # Function to compute discriminator loss\n",
        "        loss_D, fake = loss_discriminator(netD, real, netG, noise, Valid_label, Fake_label, criterion)\n",
        "\n",
        "        \n",
        "        # torch.save(fake,'test_case_GAN/fake.pt')\n",
        "        # torch.save(netD,'test_case_GAN/netD.pt')\n",
        "        # torch.save(real,'test_case_GAN/real.pt')\n",
        "        # torch.save(netG,'test_case_GAN/netG.pt')\n",
        "        # torch.save(noise,'test_case_GAN/noise.pt')\n",
        "        # torch.save(Valid_label,'test_case_GAN/Valid_label.pt')\n",
        "        # torch.save(Fake_label,'test_case_GAN/Fake_label.pt')\n",
        "        # torch.save(criterion,'test_case_GAN/criterion.pt')\n",
        "\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        loss_D.backward(retain_graph=True)\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        # Function to compute generator loss\n",
        "        loss_G = loss_generator(netD, fake, Valid_label, criterion)\n",
        "        # Calculate gradients for G\n",
        "        loss_G.backward()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\t'\n",
        "                  % (epoch, num_epochs, i, len(gan_train_loader),\n",
        "                     loss_D.item(), loss_G.item()))\n",
        "                        \n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(loss_G.item())\n",
        "        D_losses.append(loss_D.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(gan_train_loader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1\n",
        "    if epoch%10==0:\n",
        "        checkpoint = {'netG': netG.state_dict(),\n",
        "                'netD': netD.state_dict()}\n",
        "        torch.save(checkpoint, 'models/gan_{}.pt'.format(epoch))\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "torch.save(G_losses,\"G_losses.pth\")\n",
        "torch.save(D_losses,\"D_losses.pth\")\n",
        "checkpoint = {'netG': netG.state_dict(),\n",
        "              'netD': netD.state_dict()}\n",
        "torch.save(checkpoint, '/models/gan_{}.pt'.format(num_epochs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtyDyRFHXgVq"
      },
      "source": [
        "## Qualitative Visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJsFExRiOFJy"
      },
      "source": [
        "### GAN Testing (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "0lPXNqJnOEVL",
        "outputId": "759efdf1-69f0-4ec7-8d3f-3043bfeb03f6"
      },
      "outputs": [],
      "source": [
        "def load_param(num_eps):\n",
        "  model_saved = torch.load('models/gan_{}.pt'.format(num_eps))\n",
        "  netG.load_state_dict(model_saved['netG'])\n",
        "  netD.load_state_dict(model_saved['netD'])\n",
        "\n",
        "\n",
        "ep = 100\n",
        "def oneModelTest(ep):\n",
        "    load_param(ep)\n",
        "    with torch.no_grad():\n",
        "        b_size = 36\n",
        "        latentVar = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        image_6x6 = netG(latentVar)\n",
        "        images_in_grid = vutils.make_grid(image_6x6, nrow=6 , normalize=True)\n",
        "        plt.figure()\n",
        "        plt.imshow(images_in_grid.cpu().numpy().transpose(1,2,0))\n",
        "        plt.title(\"GAN Testing\")\n",
        "        plt.show()\n",
        "oneModelTest(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0LeW6TQQR2P"
      },
      "source": [
        "### Visualisation at different epochs (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i59gX9rDQYDa"
      },
      "outputs": [],
      "source": [
        "# Plot GAN generated images at different epochs during training\n",
        "for i in [20, 40, 60, 80, 100]:\n",
        "    oneModelTest(i)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUKNIzwZXp10"
      },
      "source": [
        "## Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA4kQI0la72N",
        "outputId": "ea07df85-9cf0-4172-ef02-31f291e40901"
      },
      "outputs": [],
      "source": [
        "# Resize image to this size\n",
        "image_size=64\n",
        "\n",
        "# Setting up transforms to resize and normalize \n",
        "transform=transforms.Compose([ transforms.Resize(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# Load STL-10 test set\n",
        "test_batch_size = 1\n",
        "gan_test_dataset = datasets.STL10(root='./stl10_data/', split='test', transform=transform, download=True)\n",
        "gan_test_loader = torch.utils.data.DataLoader(dataset=gan_test_dataset, batch_size=test_batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMj7_yeVRdh3"
      },
      "source": [
        "### FID score for GAN\n",
        "This part, you would be asked to compute a FID score between the real image and the generated images to evaluate the photo-realistic quality of the generated images. \n",
        "For this part, we should create three folder: \n",
        "- 1. STL_10_real_1 \n",
        "- 2. STL_10_real_2 \n",
        "- 3. STL_10_fake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppypwyiwaWD6"
      },
      "outputs": [],
      "source": [
        "! mkdir STL_10_fake STL_10_real_1 STL_10_real_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5NjebECaGM"
      },
      "source": [
        "Create the dataset to evaluate quantitative scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi6fVhPtX7nm"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Number of channels\n",
        "nc = 3\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "# Size of feature maps in generator\n",
        "ngf = 128\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 128\n",
        "# Load checkpoint\n",
        "model_saved = torch.load('models/gan_{}.pt'.format(num_epochs))\n",
        "netG = DCGAN_Generator()\n",
        "netG.load_state_dict(model_saved['netG'])\n",
        "netG.to(device)\n",
        "\n",
        "\n",
        "def denorm(x, norm_mean, norm_std):\n",
        "    y = torch.zeros(x.shape)\n",
        "    y[:,0,:,:] = x[:,0,:,:]*norm_std[0] + norm_mean[0]\n",
        "    y[:,1,:,:] = x[:,1,:,:]*norm_std[1] + norm_mean[1]\n",
        "    y[:,2,:,:] = x[:,2,:,:]*norm_std[2] + norm_mean[2]\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "evaluate_num = 1000\n",
        "\n",
        "# make the gen data set and folder\n",
        "gen_set = []\n",
        "for idx in range(evaluate_num):\n",
        "    with torch.no_grad():\n",
        "        fixed_noise = torch.randn(1, nz, 1, 1, device=device)\n",
        "        fake = netG(fixed_noise).detach().cpu()\n",
        "        fake = fake.cpu().detach()\n",
        "        fake_denorm = denorm(fake, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        # print(fake_denorm.shape)\n",
        "        gen_set.append(fake_denorm)\n",
        "        \n",
        "        # plt.imshow(np.transpose(vutils.make_grid(fake, nrow=6 ,padding=2, normalize=True),(1,2,0)), animated=True)\n",
        "        save_image(fake.squeeze(), './STL_10_fake/fake' + str(idx) + '.png', normalize=True)\n",
        "\n",
        "gen_dataset = TensorDataset(torch.cat(gen_set))\n",
        "\n",
        "# make the real data set and folder\n",
        "real_set_1 = []\n",
        "real_set_2 = []\n",
        "\n",
        "for idx, data in enumerate(gan_test_loader, 0):\n",
        "    image = data[0]\n",
        "    # print(image.shape)\n",
        "    image_denorm = denorm(image, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    if idx < evaluate_num:\n",
        "        real_set_1.append(image_denorm)\n",
        "        save_image(image.squeeze(), './STL_10_real_1/real' + str(idx) + '.png', normalize=True)\n",
        "    elif evaluate_num <= idx < 2*evaluate_num:\n",
        "        real_set_2.append(image_denorm)\n",
        "        save_image(image.squeeze(), './STL_10_real_2/real' + str(idx) + '.png', normalize=True)\n",
        "    if idx == 2*evaluate_num-1:\n",
        "        break\n",
        "\n",
        "# make the pytorch dataset\n",
        "real_dataset_1 = TensorDataset(torch.cat(real_set_1))\n",
        "real_dataset_2 = TensorDataset(torch.cat(real_set_2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9czVBsiwtCz3"
      },
      "source": [
        "#### Self-implement FID score (optional TODO)\n",
        "This optional part is for implementing FID score computation following pdf instructions. Implement \"compute_FID\" function which is the most important part in FID score computation. You could also use the the next section \"FID API\" to compute the FID score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BAHGrkSGc-cT"
      },
      "outputs": [],
      "source": [
        "# ! pip install pytorch-fid\n",
        "from pytorch_fid.inception import InceptionV3\n",
        "\n",
        "def build_feature_table(dataset, model, batch_size, dim, device):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        model: Inception network v3\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "    Output:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    '''\n",
        "    # model enter eval mode\n",
        "    model.eval()\n",
        "    # initalize the dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "    n = len(dataset)\n",
        "    idx_counter = 0\n",
        "    # feature table\n",
        "    feature_table = np.zeros((n, dim))\n",
        "\n",
        "    for i, data in tqdm.tqdm(enumerate(dataloader, 0)):\n",
        "        image = data[0].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            pred = model(image)[0]\n",
        "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "            feature_table[idx_counter:idx_counter+pred.shape[0]] = pred\n",
        "            idx_counter += len(pred)\n",
        "    \n",
        "    return feature_table\n",
        "\n",
        "def compute_stat(feature_table):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    Output:\n",
        "        mu: mean along row dimension\n",
        "        sigma: covarance matrix of dataset\n",
        "    '''\n",
        "    # compute mean and sigma based on activation table\n",
        "    mu = np.mean(feature_table, axis=0)\n",
        "    sigma = np.cov(feature_table, rowvar=False)\n",
        "\n",
        "    # print('dimension of mu', mu.shape)\n",
        "    # print('dimension of sigma', sigma.shape)\n",
        "    return mu, sigma\n",
        "\n",
        "\n",
        "def compute_FID(mu_1, sigma_1, mu_2, sigma_2, eps=1e-6):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        mu_1: mean vector we get for dataset1 \n",
        "        sigma_1: covariance matrix for dataset1\n",
        "        mu_2: mean vector we get for dataset2 \n",
        "        sigma_2: covariance matrix for dataset1\n",
        "    Output:\n",
        "        FID score: float\n",
        "    '''\n",
        "    \n",
        "    # compute mu difference\n",
        "\n",
        "    # compute square root of Sigma1*Sigma2 using \"linalg.sqrtm\" from scipy \n",
        "    # please name the resulting matrix as covmean\n",
        "    covmean = linalg.sqrtm(sigma_1@sigma_2)\n",
        "\n",
        "    # The following block take care of imagionary part of covmean \n",
        "    #################################################################\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma_1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma_1 + offset).dot(sigma_2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "    #################################################################\n",
        "\n",
        "    # compute FID score, based on eqution.(10) in pdf FID part.\n",
        "    FID_score = np.linalg.norm(mu_1 - mu_2)**2 + np.trace(sigma_1 + sigma_2 - 2*covmean)\n",
        "\n",
        "    return FID_score\n",
        "\n",
        "def FID(dataset_1, dataset_2, device, batch_size=64, dim=2048, block_idx = 3):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        dataset_1: pytorch dataset\n",
        "        dataset_2: pytorch dataset\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        block_idx: the block stage index we want to use in inception module\n",
        "    Output:\n",
        "        FID_score: float\n",
        "    '''\n",
        "    # load InveptionV3 model\n",
        "    model = InceptionV3([block_idx]).to(device)\n",
        "\n",
        "    ## build up the feature table \n",
        "    feature_table_1 = build_feature_table(dataset_1, model, batch_size, dim, device)\n",
        "    feature_table_2 = build_feature_table(dataset_2, model, batch_size, dim, device)\n",
        "\n",
        "\n",
        "    ## compute mu, sigma for dataset 1&2\n",
        "    mu_1, sigma_1 = compute_stat(feature_table_1)\n",
        "    mu_2, sigma_2 = compute_stat(feature_table_2)\n",
        "\n",
        "\n",
        "    ## FID score computation\n",
        "    FID_score = compute_FID(mu_1, sigma_1, mu_2, sigma_2, eps=1e-6)\n",
        "    \n",
        "    \n",
        "    return FID_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DLPWmvylsPPZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16it [00:02,  6.51it/s]\n",
            "16it [00:02,  7.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID_score between real_dataset_1 and itself: -2.1959189191914953e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16it [00:02,  6.48it/s]\n",
            "16it [00:02,  6.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID_score between real_dataset_1 and real_dataset_2: 37.668882634287456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16it [00:02,  6.42it/s]\n",
            "16it [00:02,  6.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID_score between real_dataset_1 and generated image set: 212.56911313055934\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# compute FID using FID function you implemented above\n",
        "FID_score = FID(real_dataset_1, real_dataset_1, device)\n",
        "print('FID_score between real_dataset_1 and itself:', FID_score)\n",
        "\n",
        "FID_score = FID(real_dataset_1, real_dataset_2, device)\n",
        "print('FID_score between real_dataset_1 and real_dataset_2:', FID_score)\n",
        "\n",
        "FID_score = FID(real_dataset_1, gen_dataset, device)\n",
        "print('FID_score between real_dataset_1 and generated image set:', FID_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGuB5sUctmbA"
      },
      "source": [
        "#### FID API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "RRpPUoLZc9iP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID:  -2.2007190409567556e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|▌         | 1/20 [00:03<01:06,  3.52s/it]\n",
            " 10%|█         | 2/20 [00:03<00:27,  1.52s/it]\n",
            " 15%|█▌        | 3/20 [00:03<00:15,  1.13it/s]\n",
            " 20%|██        | 4/20 [00:03<00:09,  1.70it/s]\n",
            " 25%|██▌       | 5/20 [00:04<00:06,  2.37it/s]\n",
            " 30%|███       | 6/20 [00:04<00:04,  3.11it/s]\n",
            " 35%|███▌      | 7/20 [00:04<00:03,  3.88it/s]\n",
            " 40%|████      | 8/20 [00:04<00:02,  4.64it/s]\n",
            " 45%|████▌     | 9/20 [00:04<00:02,  5.32it/s]\n",
            " 50%|█████     | 10/20 [00:04<00:01,  5.97it/s]\n",
            " 55%|█████▌    | 11/20 [00:04<00:01,  6.48it/s]\n",
            " 60%|██████    | 12/20 [00:04<00:01,  6.92it/s]\n",
            " 65%|██████▌   | 13/20 [00:05<00:00,  7.25it/s]\n",
            " 70%|███████   | 14/20 [00:05<00:00,  7.55it/s]\n",
            " 75%|███████▌  | 15/20 [00:05<00:00,  7.75it/s]\n",
            " 80%|████████  | 16/20 [00:05<00:00,  7.91it/s]\n",
            " 85%|████████▌ | 17/20 [00:05<00:00,  7.98it/s]\n",
            " 90%|█████████ | 18/20 [00:05<00:00,  8.07it/s]\n",
            " 95%|█████████▌| 19/20 [00:05<00:00,  8.18it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  8.28it/s]\n",
            "100%|██████████| 20/20 [00:06<00:00,  3.15it/s]\n",
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|▌         | 1/20 [00:02<00:55,  2.93s/it]\n",
            " 10%|█         | 2/20 [00:03<00:23,  1.28s/it]\n",
            " 15%|█▌        | 3/20 [00:03<00:12,  1.32it/s]\n",
            " 20%|██        | 4/20 [00:03<00:08,  1.97it/s]\n",
            " 25%|██▌       | 5/20 [00:03<00:05,  2.70it/s]\n",
            " 30%|███       | 6/20 [00:03<00:04,  3.47it/s]\n",
            " 35%|███▌      | 7/20 [00:03<00:03,  4.27it/s]\n",
            " 40%|████      | 8/20 [00:03<00:02,  5.03it/s]\n",
            " 45%|████▌     | 9/20 [00:03<00:01,  5.69it/s]\n",
            " 50%|█████     | 10/20 [00:04<00:01,  6.29it/s]\n",
            " 55%|█████▌    | 11/20 [00:04<00:01,  6.77it/s]\n",
            " 60%|██████    | 12/20 [00:04<00:01,  7.14it/s]\n",
            " 65%|██████▌   | 13/20 [00:04<00:00,  7.47it/s]\n",
            " 70%|███████   | 14/20 [00:04<00:00,  7.69it/s]\n",
            " 75%|███████▌  | 15/20 [00:04<00:00,  7.90it/s]\n",
            " 80%|████████  | 16/20 [00:04<00:00,  8.01it/s]\n",
            " 85%|████████▌ | 17/20 [00:04<00:00,  8.13it/s]\n",
            " 90%|█████████ | 18/20 [00:05<00:00,  8.19it/s]\n",
            " 95%|█████████▌| 19/20 [00:05<00:00,  8.26it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  8.27it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  3.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID:  37.682589998227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|▌         | 1/20 [00:04<01:26,  4.54s/it]\n",
            " 10%|█         | 2/20 [00:04<00:35,  1.94s/it]\n",
            " 15%|█▌        | 3/20 [00:04<00:18,  1.12s/it]\n",
            " 20%|██        | 4/20 [00:04<00:11,  1.38it/s]\n",
            " 25%|██▌       | 5/20 [00:05<00:07,  1.96it/s]\n",
            " 30%|███       | 6/20 [00:05<00:05,  2.64it/s]\n",
            " 35%|███▌      | 7/20 [00:05<00:03,  3.38it/s]\n",
            " 40%|████      | 8/20 [00:05<00:02,  4.17it/s]\n",
            " 45%|████▌     | 9/20 [00:05<00:02,  4.89it/s]\n",
            " 50%|█████     | 10/20 [00:05<00:01,  5.58it/s]\n",
            " 55%|█████▌    | 11/20 [00:05<00:01,  6.21it/s]\n",
            " 60%|██████    | 12/20 [00:05<00:01,  6.75it/s]\n",
            " 65%|██████▌   | 13/20 [00:06<00:00,  7.18it/s]\n",
            " 70%|███████   | 14/20 [00:06<00:00,  7.51it/s]\n",
            " 75%|███████▌  | 15/20 [00:06<00:00,  7.78it/s]\n",
            " 80%|████████  | 16/20 [00:06<00:00,  7.94it/s]\n",
            " 85%|████████▌ | 17/20 [00:06<00:00,  8.12it/s]\n",
            " 90%|█████████ | 18/20 [00:06<00:00,  8.22it/s]\n",
            " 95%|█████████▌| 19/20 [00:06<00:00,  8.30it/s]\n",
            "100%|██████████| 20/20 [00:06<00:00,  8.36it/s]\n",
            "100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n",
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|▌         | 1/20 [00:02<00:54,  2.85s/it]\n",
            " 10%|█         | 2/20 [00:02<00:22,  1.25s/it]\n",
            " 15%|█▌        | 3/20 [00:03<00:12,  1.35it/s]\n",
            " 20%|██        | 4/20 [00:03<00:07,  2.01it/s]\n",
            " 25%|██▌       | 5/20 [00:03<00:05,  2.74it/s]\n",
            " 30%|███       | 6/20 [00:03<00:03,  3.52it/s]\n",
            " 35%|███▌      | 7/20 [00:03<00:03,  4.29it/s]\n",
            " 40%|████      | 8/20 [00:03<00:02,  5.02it/s]\n",
            " 45%|████▌     | 9/20 [00:03<00:01,  5.68it/s]\n",
            " 50%|█████     | 10/20 [00:04<00:01,  6.24it/s]\n",
            " 55%|█████▌    | 11/20 [00:04<00:01,  6.72it/s]\n",
            " 60%|██████    | 12/20 [00:04<00:01,  7.14it/s]\n",
            " 65%|██████▌   | 13/20 [00:04<00:00,  7.44it/s]\n",
            " 70%|███████   | 14/20 [00:04<00:00,  7.69it/s]\n",
            " 75%|███████▌  | 15/20 [00:04<00:00,  7.92it/s]\n",
            " 80%|████████  | 16/20 [00:04<00:00,  8.06it/s]\n",
            " 85%|████████▌ | 17/20 [00:04<00:00,  8.18it/s]\n",
            " 90%|█████████ | 18/20 [00:04<00:00,  8.26it/s]\n",
            " 95%|█████████▌| 19/20 [00:05<00:00,  8.32it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  8.35it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID:  212.68497767388413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|▌         | 1/20 [00:03<00:58,  3.09s/it]\n",
            " 10%|█         | 2/20 [00:03<00:24,  1.35s/it]\n",
            " 15%|█▌        | 3/20 [00:03<00:13,  1.26it/s]\n",
            " 20%|██        | 4/20 [00:03<00:08,  1.89it/s]\n",
            " 25%|██▌       | 5/20 [00:03<00:05,  2.61it/s]\n",
            " 30%|███       | 6/20 [00:03<00:04,  3.38it/s]\n",
            " 35%|███▌      | 7/20 [00:03<00:03,  4.15it/s]\n",
            " 40%|████      | 8/20 [00:03<00:02,  4.90it/s]\n",
            " 45%|████▌     | 9/20 [00:04<00:01,  5.56it/s]\n",
            " 50%|█████     | 10/20 [00:04<00:01,  6.12it/s]\n",
            " 55%|█████▌    | 11/20 [00:04<00:01,  6.63it/s]\n",
            " 60%|██████    | 12/20 [00:04<00:01,  7.04it/s]\n",
            " 65%|██████▌   | 13/20 [00:04<00:00,  7.35it/s]\n",
            " 70%|███████   | 14/20 [00:04<00:00,  7.61it/s]\n",
            " 75%|███████▌  | 15/20 [00:04<00:00,  7.81it/s]\n",
            " 80%|████████  | 16/20 [00:04<00:00,  7.95it/s]\n",
            " 85%|████████▌ | 17/20 [00:05<00:00,  8.06it/s]\n",
            " 90%|█████████ | 18/20 [00:05<00:00,  8.12it/s]\n",
            " 95%|█████████▌| 19/20 [00:05<00:00,  8.15it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  8.22it/s]\n",
            "100%|██████████| 20/20 [00:06<00:00,  3.33it/s]\n",
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  5%|▌         | 1/20 [00:02<00:56,  2.96s/it]\n",
            " 10%|█         | 2/20 [00:03<00:23,  1.30s/it]\n",
            " 15%|█▌        | 3/20 [00:03<00:13,  1.30it/s]\n",
            " 20%|██        | 4/20 [00:03<00:08,  1.94it/s]\n",
            " 25%|██▌       | 5/20 [00:03<00:05,  2.67it/s]\n",
            " 30%|███       | 6/20 [00:03<00:04,  3.44it/s]\n",
            " 35%|███▌      | 7/20 [00:03<00:03,  4.22it/s]\n",
            " 40%|████      | 8/20 [00:03<00:02,  4.94it/s]\n",
            " 45%|████▌     | 9/20 [00:03<00:01,  5.57it/s]\n",
            " 50%|█████     | 10/20 [00:04<00:01,  6.11it/s]\n",
            " 55%|█████▌    | 11/20 [00:04<00:01,  6.60it/s]\n",
            " 60%|██████    | 12/20 [00:04<00:01,  7.00it/s]\n",
            " 65%|██████▌   | 13/20 [00:04<00:00,  7.28it/s]\n",
            " 70%|███████   | 14/20 [00:04<00:00,  7.52it/s]\n",
            " 75%|███████▌  | 15/20 [00:04<00:00,  7.73it/s]\n",
            " 80%|████████  | 16/20 [00:04<00:00,  7.87it/s]\n",
            " 85%|████████▌ | 17/20 [00:04<00:00,  7.96it/s]\n",
            " 90%|█████████ | 18/20 [00:05<00:00,  8.03it/s]\n",
            " 95%|█████████▌| 19/20 [00:05<00:00,  8.11it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  8.18it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  3.42it/s]\n"
          ]
        }
      ],
      "source": [
        "# ! pip install pytorch-fid\n",
        "! python -m pytorch_fid STL_10_real_1 STL_10_real_1 \n",
        "! python -m pytorch_fid STL_10_real_1 STL_10_real_2 \n",
        "! python -m pytorch_fid STL_10_real_1 STL_10_fake \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWmrarYMnlzW"
      },
      "source": [
        "If you implement the self-implement FID version. You would notice a negligible difference in value. Because the API package compute this value by saving and reading tensor from .png files. Accuracy problem could happen in the middle of transation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP32sx7RuiMa"
      },
      "source": [
        "### IS score for GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOSYYLCoNAP"
      },
      "source": [
        "#### Self-implement IS score (optional)\n",
        "\n",
        "You could copy and paste this function from VAE part if you have implement that part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DtkK_n62C70"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.inception import inception_v3\n",
        "\n",
        "def build_feature_table(dataset, model, batch_size, dim, device, upsample):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        model: Inception network v3\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        upsample: Inception network v3 only support input with resolution 299x299\n",
        "    Output:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    '''\n",
        "    # model enter eval mode\n",
        "    model.eval()\n",
        "    # initalize the dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "    n = len(dataset)\n",
        "    idx_counter = 0\n",
        "    # feature table\n",
        "    feature_table = np.zeros((n, dim))\n",
        "\n",
        "    for i, data in tqdm.tqdm(enumerate(dataloader, 0)):\n",
        "        image = data[0].to(device)\n",
        "        up = nn.Upsample(size=(299, 299), mode='bilinear')\n",
        "        with torch.no_grad():\n",
        "            if upsample == True:\n",
        "                # upsample to 299x299 resolution\n",
        "                pred = up(image)\n",
        "            pred = model(pred)\n",
        "            # print(pred.shape)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "            \n",
        "            feature_table[idx_counter:idx_counter+pred.shape[0]] = pred\n",
        "            idx_counter += len(pred)\n",
        "    assert feature_table.shape == (n, dim)\n",
        "    return feature_table\n",
        "\n",
        "def compute_IS(feature_table):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    Output:\n",
        "        IS_score: float\n",
        "    '''\n",
        "    ################################\n",
        "    # Please fill in your code here:\n",
        "\n",
        "    ################################\n",
        "    \n",
        "    return IS_score\n",
        "\n",
        "def IS(dataset, device, upsample=True, batch_size=64, dim=1000):\n",
        "    '''\n",
        "    Argms: \n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        upsample: Inception network v3 only support input with resolution 299x299\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "    Output:\n",
        "        IS_score: float\n",
        "    '''\n",
        "    \n",
        "    # load InveptionV3 model\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "\n",
        "    ## build up the feature table \n",
        "    feature_table = build_feature_table(dataset, model, batch_size, dim, device, upsample)\n",
        "\n",
        "    ## IS score computation\n",
        "    IS_score = compute_IS(feature_table)\n",
        "    \n",
        "    \n",
        "    return IS_score\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u5A2diuogJp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# compute IS score\n",
        "real_set_IS_score = IS(real_dataset_1, device, upsample = True)\n",
        "gen_set_IS_score = IS(gen_dataset, device, upsample = True)\n",
        "print('real images IS_score:', real_set_IS_score)\n",
        "print('generated images IS_score:', gen_set_IS_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzN3WLMvus1t"
      },
      "source": [
        "#### IS API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNkIvNvKpKSX"
      },
      "outputs": [],
      "source": [
        "from inception_score import inception_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print('IS score for real data set 1:')\n",
        "print(inception_score(real_dataset_1, cuda=True, batch_size=64, resize=True, splits=1))\n",
        "print('IS score for generated data set:')\n",
        "print(inception_score(gen_dataset, cuda=True, batch_size=64, resize=True, splits=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlsvP0JfPN8c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CIS680_Fall2020_HW5_VAE_GAN_template.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
